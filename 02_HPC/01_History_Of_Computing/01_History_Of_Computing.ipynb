{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f261a570",
   "metadata": {},
   "source": [
    "# A Brief History of Computing\n",
    "\n",
    "## What was the first electronic digital computer?\n",
    "\n",
    "The first **electronic digital computer used in real operations** was **Colossus**, built at Bletchley Park (U.K.) in **1943** to crack the German **Lorenz cipher**. It was designed by **Tommy Flowers**, an electronics engineer from the UK Post Office Research Station.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"ColossusRebuild_11.jpg\" width=\"500\"></td>\n",
    "  <td><img src=\"Tommy_Flowers.jpg\" width=\"500\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "This followed earlier work by **Alan Turing** and others, also at Bletchley Park, who developed the **electro-mechanical Bombe** to break the **Enigma cipher** beginning in **1940**.\n",
    "\n",
    "You’ve probably heard of **Turing**, but not **Flowers**. Why?\n",
    "\n",
    "- **Turing is the father of modern computer science and digital computing theory.**  \n",
    "  He defined the **Turing Machine** in **1936**, which formalized what it means for a machine to compute anything at all. Every programmable digital computer is, in theory, a realization of a Turing Machine.\n",
    "\n",
    "- **Flowers is the father of practical electronic digital computing.**  \n",
    "  He proved that large-scale electronic computers using thousands of vacuum tubes could be built and run reliably in continuous operation.\n",
    "\n",
    "---\n",
    "\n",
    "## Other Key Founders of Modern Computing\n",
    "\n",
    "Other key figures completed the intellectual and engineering framework of modern computers:\n",
    "\n",
    "- **John von Neumann**  \n",
    "  In **1945**, he defined the **stored-program computer architecture** (now called the *von Neumann architecture*), in which **instructions and data share the same memory**.  \n",
    "  This design still underlies almost all modern computers.  \n",
    "  He was also a key contributor to the conceptual redesign of **ENIAC**, the first **general-purpose electronic digital computer** built in the USA.\n",
    "\n",
    "- **Grace Hopper**  \n",
    "  In **1952**, she created the **first working compiler**, allowing symbolic, human-readable instructions to be translated automatically into machine code.  \n",
    "  This was a revolutionary step: computers became **human-friendly**, and programmers no longer had to work exclusively in raw machine code.  \n",
    "  She later led the development of **COBOL (1959)** and popularized the term **“computer bug.”**\n",
    "\n",
    "---\n",
    "\n",
    "## The Four Founders of Modern Digital Computing (Summary)\n",
    "\n",
    "- **Turing** → What *computation is* (theory)\n",
    "- **Flowers** → How to *build electronic computers* (hardware)\n",
    "- **von Neumann** → How computers are *organized* (architecture)\n",
    "- **Hopper** → How humans *program computers* (software)\n",
    "---\n",
    "\n",
    "## Moore's Law\n",
    "\n",
    "Your desktop computer today is phenomenally powerful. This is a consequence of **Moore's Law** — the observation that the number of transistors we can put on an integrated circuit roughly doubles every ~2 years.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"mooreslaw.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "This graph starts in 1970, because that is around the time when computers began to be used in Seismology.\n",
    "\n",
    "---\n",
    "\n",
    "## 1970: Centralized Computing\n",
    "\n",
    "In 1970, a university graduate student in seismology who wanted to run computations would almost certainly be using a **shared institutional mainframe or minicomputer**, rather than any personal or laboratory-owned computer, and those computations were used **primarily for earthquake location and magnitude determination**. Phase arrival times and amplitudes were first **read by hand from analog helicorder drum records**, measured with rulers and magnifiers, and then entered as numeric values for processing. Programs—typically written in **FORTRAN**—were prepared on paper and transferred onto **punch cards**, which were submitted to the university computing center for **batch processing** on systems such as an IBM System/360, a CDC mainframe, or a DEC PDP-10. Hypocenter solutions were computed using early **Geiger-type location algorithms**, and shortly thereafter using emerging community-standard codes such as **HYPO71 (released in 1971)**, which would soon become the dominant global earthquake location program. Turnaround times ranged from hours to days, making debugging slow and highly iterative. Computers were rarely used for direct waveform analysis; instead, their dominant role was to convert manually picked phase times into **hypocenter solutions and standardized magnitudes**, with results returned as printed tables or pen-plotter graphics rather than on screens.\n",
    "\n",
    "---\n",
    "\n",
    "## 1977: Personal (Desktop) Computer Revolution\n",
    "\n",
    "## 1977 — The Birth of the Home Desktop Computer (and What It Meant for Seismology)\n",
    "\n",
    "The first **commercially successful desktop computers** reached the mass market in **1977**, most notably the **Apple II** and the **Commodore PET**, which together created the modern home-computer industry and defined what a “personal computer” would become.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"apple2.jpg\" width=\"500\"></td>\n",
    "  <td><img src=\"commodorepet.jpg\" width=\"500\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "These early microcomputers introduced, for the first time, the idea that **a single person could own and operate a fully programmable computer on a desktop**. Universities rapidly adopted them for **teaching programming, introductory numerical methods, word processing, and spreadsheets**, even though they were not yet powerful enough to replace institutional mainframes or minicomputers for serious research computing.\n",
    "\n",
    "From a **seismological perspective**, this period still belonged firmly to the **mainframe and minicomputer era** for real data processing. Earthquake location, magnitude calculation, and travel-time modeling continued to run on IBM, CDC, and DEC systems using **FORTRAN codes** derived from Geiger-type algorithms and, soon after, standardized programs such as **HYPO71 (released 1971)**. However, the spread of desktop microcomputers fundamentally changed how students learned to program and analyze data. The formal standardization of **FORTRAN 77 in 1977** was especially important, as it became the dominant scientific programming language across both mainframes and emerging desktop systems, ensuring long-term portability of seismological codes across computing platforms for decades to come.\n",
    "\n",
    "Thus, while **Apple II–class machines transformed education and personal productivity**, real-time seismic monitoring, waveform processing, and routine earthquake catalogs still depended on **large shared computers**. The personal desktop had arrived—but the scientific infrastructure had not yet followed.\n",
    "\n",
    "---\n",
    "\n",
    "## 1980s: IBM PCs, the Attack of the Clones, and the Birth of Digital Seismology\n",
    "\n",
    "The **IBM Personal Computer (IBM PC)** was introduced in **1981** using an **Intel 8088** processor and IBM’s newly defined open hardware architecture, marking a pivotal shift toward standardized personal computing. By choosing off-the-shelf components and publishing technical specifications, IBM unintentionally enabled a vast ecosystem of **x86-based “PC clones.”** In **1982–1983**, companies such as Compaq demonstrated the first fully compatible clones through clean-room BIOS reverse-engineering, followed rapidly by dozens of manufacturers producing cheaper, faster machines. By the **mid-to-late 1980s**, IBM-compatible PCs had become the dominant platform in business, government, and universities, consolidating software development around **MS-DOS** and later **Windows**, and establishing **x86** as the world’s primary desktop computing architecture for the next four decades.\n",
    "\n",
    "By the **mid-1980s**, a graduate student in seismology would encounter a hybrid computing environment combining **departmental minicomputers** and emerging **desktop microcomputers**. Core seismic processing—now increasingly including **automated event detection, phase picking, earthquake location, and magnitude calculation**—was typically performed on **PDP-11 and VAX-class systems** running UNIX or VMS, accessed through text terminals. At the same time, **IBM PCs and early clones** were becoming common on the desktop for coding, plotting, and report writing using FORTRAN compilers and early graphics packages.\n",
    "\n",
    "Crucially, this decade marked the true transition from **analog to digital seismology**. **Analog-to-digital converters (ADCs)** were installed directly at observatories, allowing **continuous digital waveform streams** to be processed in real time for the first time. However, this transition was fundamentally constrained by **severe storage and telemetry limitations**. Even modest continuous digital recording generated data volumes that were **too expensive to archive**, as disk capacity was small and magnetic tape was slow, costly, and labor-intensive. Low-bandwidth radio and leased telephone telemetry further limited what could be transmitted and retained.\n",
    "\n",
    "These constraints made **automated STA/LTA (short-term average / long-term average) detectors central to digital seismology**, not only as real-time alerting tools but as **essential data-reduction mechanisms**. STA/LTA algorithms ran continuously on streaming telemetry, **alerting seismologists to new events in near-real time** and **selectively triggering short waveform windows for permanent digital storage**. In this way, the entire workflow of **event detection → phase picking → hypocenter location → magnitude calculation** became, for the first time, a unified **digital signal-processing pipeline**.\n",
    "\n",
    "Throughout much of the 1980s, **analog drum helicorders were still operated in parallel** with early digital systems, producing a mixed observational environment in which human analysts continued to visually interpret paper records while computers increasingly watched the Earth automatically. The 1980s therefore represent the moment when seismic monitoring crossed the threshold from **manual, observer-driven practice to computer-driven, real-time digital observatories**, setting the stage for the fully continuous digital networks and archives of the 1990s and beyond.\n",
    "\n",
    "---\n",
    "\n",
    "## 1990s: UNIX Workstations, PCs, the Internet, and the Era of Continuous Digital Seismology\n",
    "\n",
    "The **1990s** marked the full maturation of **desktop computing and global networking**. UNIX workstations from vendors such as Sun, Silicon Graphics, and DEC became common on scientific desktops in the early part of the decade, offering powerful floating-point performance, advanced graphics, and true multi-user networking. In parallel, **IBM-compatible PCs** based on rapidly advancing x86 processors gained hardware floating-point units, large RAM, and fast local disks, making them increasingly viable for technical computing. By the **mid-to-late 1990s**, inexpensive **Linux-based PCs** began to displace proprietary UNIX workstations as the dominant scientific desktop. At the same time, the rapid global expansion of the **Internet and Ethernet networking** enabled routine file transfer, remote logins, and near-real-time data exchange between institutions for the first time.\n",
    "\n",
    "Educationally, this decade saw a clear **two-tier computing culture** emerge at universities. **Undergraduate students** were typically trained on **IBM PCs or Macintoshes** for **word processing, drawing, spreadsheets, presentations, and basic programming**, with **email and Internet use becoming routine by the mid-1990s**. In contrast, **graduate students in the physical sciences** were expected to become fluent in **UNIX environments**, command-line workflows, and **compiled programming languages such as C and FORTRAN 77**, including manual code compilation and linking. **Interpreted languages were rarely used for serious numerical work** at the time because **CPU speed and memory were still the dominant bottlenecks**, making execution efficiency far more critical than software development speed. Scientific computing therefore emphasized **optimized compiled code** to extract maximum performance from limited hardware.\n",
    "\n",
    "For seismology, these hardware and networking advances triggered a **fundamental transformation from trigger-based digital monitoring to fully continuous digital observatories**. By the early 1990s, the combination of **cheap hard disks, affordable high-capacity tape systems (e.g., DAT and DLT), and improved telemetry bandwidth** made it economically feasible to **archive continuous waveform data at scale**. Seismic networks therefore transitioned from storing only triggered event windows to maintaining **continuous digital archives**, preserving not only earthquakes but also previously unrecordable background signals such as tremor, noise, and long-period deformation.\n",
    "\n",
    "At the same time, **real-time automated processing pipelines matured**. Systems such as **Earthworm, Antelope, and related network frameworks** integrated **continuous data acquisition, STA/LTA detection, phase picking, event association, hypocenter location, magnitude calculation, and database archiving** into unified real-time workflows. Detection algorithms were no longer used primarily as storage filters but instead became components of **fully automated digital monitoring systems** feeding live catalogs, alarms, and early web-based displays.\n",
    "\n",
    "By the end of the decade, most regional and national seismic networks had become **continuously digital, networked, and largely automated**, with analysts shifting from watching paper drums to **interacting with live waveform displays, digital spectrograms, and real-time catalogs on networked workstations and PCs**. The **1990s thus represent the decade in which seismology completed its transition into a modern, continuously recorded, networked digital science**, laying the technological and cultural foundation for today’s real-time global monitoring systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 2000s: Linux Clusters, Open-Source Software, and the Rise of Large-Scale Computational Seismology\n",
    "\n",
    "The **2000s** marked a decisive shift from proprietary scientific workstations to **commodity Linux-based computing**. By the early part of the decade, inexpensive **x86 PCs running Linux** had largely replaced remaining UNIX workstations as the standard scientific desktop. At the same time, universities and research centers began assembling **Linux clusters** using off-the-shelf hardware connected by fast Ethernet or early InfiniBand networks, enabling parallel processing at a fraction of the cost of traditional supercomputers. Disk storage capacity expanded dramatically with the advent of inexpensive IDE and later SATA drives, making **continuous multi-year waveform archives routine** for the first time.\n",
    "\n",
    "This decade also saw the explosion of **open-source scientific software** in seismology. Community packages for **data acquisition, real-time processing, visualization, and archiving** became widely available and rapidly adopted, replacing many locally maintained legacy codes. Digital seismic networks were now fully **Internet-connected**, enabling near-instant data sharing between observatories, national data centers, and global archives. Continuous waveform data, metadata, and earthquake catalogs began to be distributed in standardized formats, laying the groundwork for modern interoperable seismological infrastructure.\n",
    "\n",
    "From a research perspective, the 2000s enabled true **computationally intensive waveform modeling and large-scale data analysis** on university desktops and small clusters. Tasks such as **finite-difference and spectral-element wave propagation modeling, frequency–wavenumber (f–k) simulations, waveform cross-correlation, and network-scale inverse problems** became routine rather than exceptional. Development still emphasized **compiled languages (C, C++, FORTRAN 77/90)** for performance-critical components, but higher-level environments such as **MATLAB and emerging Python numerical libraries** increasingly entered research workflows for visualization, post-processing, and experimentation as computing power grew.\n",
    "\n",
    "Operationally, real-time seismic monitoring systems became **continuous, automated, and web-facing**. Analysts now interacted with **live waveform browsers, spectrograms, and continuously updating event catalogs** through graphical interfaces and early web dashboards. Automated detection, association, and location pipelines ran continuously in the background, while humans focused increasingly on **interpretation, quality control, and hazard communication** rather than basic signal discovery.\n",
    "\n",
    "By the end of the **2000s**, seismology had fully transitioned into a **data-intensive, networked, computational science**, characterized by continuous global waveform archives, open-source processing ecosystems, Linux-based computing infrastructure, and routine large-scale numerical modeling—setting the stage for the machine-learning, cloud-based, and citizen-science revolutions that would follow in the 2010s.\n",
    "\n",
    "---\n",
    "\n",
    "## 2010s: Multi-Core Computing, Big Data Storage, High-Speed Internet, and the IRIS Data Revolution\n",
    "\n",
    "The **2010s** were defined by the convergence of **multi-core desktop and server CPUs, inexpensive multi-terabyte storage, and pervasive high-speed Internet connectivity**. Memory and disk capacity increased by orders of magnitude, solid-state storage became routine, and academic networks achieved sustained high-throughput global data transfer. These advances transformed computing from a resource-limited activity into a **data-intensive scientific enterprise**, in which storage, bandwidth, and compute power were no longer rare commodities but standard infrastructure.\n",
    "\n",
    "For seismology, the most profound institutional change of the decade was the rise of the **IRIS Data Management Center (IRIS DMC)** as the **central global hub for seismic waveform data, metadata, and earthquake catalogs**. Prior to this era, observatories and regional networks typically **stored their data locally**, and researchers had to **contact individual institutions directly** to request waveform segments and catalogs. With the maturation of IRIS DMC infrastructure in the 2010s—made possible by **cheap large-scale disk storage and fast Internet links**—data-producing networks could **continuously stream and archive real-time data to a centralized, professionally managed facility**, while researchers gained **a single global portal** for standardized data discovery and retrieval. This fundamentally shifted seismology from a **distributed, institution-by-institution access model** to a **globally centralized, web-based data ecosystem**.\n",
    "\n",
    "This data revolution also reshaped the **real-time monitoring software landscape**. Long-standing systems such as **Earthworm** and **Antelope**, both originating in the **1990s**, continued to dominate many operational networks but evolved in different directions. **Earthworm**, developed at the USGS as a fast, modular real-time detection system, increasingly incorporated **database-driven catalog production and archival tools** to compete with Antelope’s long-standing strengths in integrated waveform–metadata–catalog management. In contrast, **Antelope**, developed commercially by BRTT, retained its position as a highly integrated **waveform–metadata–catalog–database platform** for large national and regional networks. During the **2010s**, the open-source **SeisComP** system developed rapidly as a powerful, modular alternative to both proprietary and legacy platforms, and was widely adopted by national and regional networks for **end-to-end real-time acquisition, processing, association, location, and alerting**.\n",
    "\n",
    "Equally transformative for research was the emergence and rapid adoption of **ObsPy**, a comprehensive, object-oriented seismic toolbox for **Python**. ObsPy provided standardized access to waveform data, metadata, catalogs, signal processing routines, and network services within a single, extensible framework. This enabled researchers—for the first time—to **rapidly prototype complete seismic processing pipelines**, from data discovery and download through filtering, detection, measurement, and visualization, using high-level scripted workflows rather than large monolithic compiled codes. The combination of **IRIS-centered global data access and ObsPy-based programmable workflows** dramatically lowered the barrier to entry for large-scale, reproducible seismological research.\n",
    "\n",
    "The net effect of these changes was a profound shift in research **scale and style**. Instead of working with **small, locally curated datasets**, investigators could now routinely analyze **continental-to-global waveform archives spanning thousands of stations and decades of time**. Studies of **ambient noise tomography, global wavefield imaging, network-wide tremor detection, and massive statistical earthquake analyses** became standard research practice rather than exceptional efforts.\n",
    "\n",
    "By the end of the decade, seismology had entered a new operational regime in which **computing power, storage capacity, and network bandwidth were no longer the primary limiting factors**. Instead, the dominant challenges increasingly became **data volume, algorithmic scalability, software sustainability, and reproducibility**—marking the full transition of the field into the modern **big-data era of Earth science**.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Box: Earthworm vs Antelope vs SeisComP (Operational Seismology, 1990s–2010s)\n",
    "\n",
    "| System | Origin & Era | Core Strengths | Typical Deployment | Key Limitations |\n",
    "|--------|---------------|----------------|---------------------|------------------|\n",
    "| **Earthworm** | Developed at USGS in the **1990s** | Fast real-time detection, modular ring-based architecture, low-latency triggering | Small to medium regional networks, volcano observatories, research testbeds | Historically weak database integration; catalog and archive tools added later to compete with Antelope |\n",
    "| **Antelope** | Developed by BRTT in the **1990s** as a commercial system | Fully integrated **waveform–metadata–catalog–database** architecture, strong archival and QC tools | Large national and regional networks, long-term operational archives | Proprietary licensing; higher cost and administrative overhead |\n",
    "| **SeisComP** | Open-source system emerging strongly in the **2010s** | End-to-end acquisition, detection, association, location, alerting; highly modular; native FDSN services | National networks, regional observatories, cloud and distributed deployments | Newer ecosystem during early adoption; migration effort from legacy systems |\n",
    "\n",
    "**Key institutional trend of the 2010s:**  \n",
    "Earthworm expanded toward **database-driven catalog production** to compete with Antelope’s long-standing integrated data model, while **SeisComP emerged as a modern, open-source alternative** capable of replacing both legacy proprietary and research-focused systems for fully integrated real-time network operations.\n",
    "\n",
    "---\n",
    "\n",
    "## 2020s: Cloud Computing, GPUs, AI/ML, ARM Processors, and the Era of Continuous Global Analysis\n",
    "\n",
    "The **2020s** have been defined by the convergence of **cloud computing, GPU acceleration, machine learning, and energy-efficient ARM-based processors** with the already mature landscape of high-speed networking and low-cost massive storage. Scientific computing is no longer tied primarily to local desktops or even institutional clusters; instead, elastic **on-demand cloud resources** allow researchers to scale computation dynamically from a single workstation to thousands of distributed cores. At the same time, **GPUs** have become standard for large numerical workloads, while **ARM processors**—long dominant in mobile devices—have entered high-performance laptops, desktops, and servers due to their exceptional performance-per-watt. Solid-state storage, object storage, and multi-petabyte archives are now routine components of research infrastructure.\n",
    "\n",
    "For seismology, these hardware trends have enabled a shift from **large-scale data access to large-scale computation on demand**. Continuous global waveform archives—already centralized through infrastructures such as IRIS—can now be processed **at planetary scale**, with analyses that span **decades of continuous data from tens of thousands of stations** becoming routine rather than exceptional. Tasks that were once limited to small geographic regions—such as dense cross-correlation, ambient noise tomography, or network-wide detection—can now be executed globally using cloud or GPU-accelerated workflows.\n",
    "\n",
    "The **rise of machine learning and artificial intelligence** has been the most visible methodological change of the decade. Neural-network–based phase pickers, detectors, classifiers, and event associators now operate at **continental to global scale**, often outperforming traditional rule-based algorithms in both sensitivity and consistency. These models depend directly on the 2010s data revolution: without centralized archives, fast networks, and massive storage, **training data-hungry ML models would not be possible**. As a result, the computational bottleneck has shifted away from data access and toward **model training, validation, interpretability, and reproducibility**.\n",
    "\n",
    "At the same time, **real-time processing has expanded outward toward the network edge**. Affordable, low-power multi-core systems—often ARM-based—now support local buffering, preprocessing, and intelligent triggering near sensors, while cloud infrastructure handles aggregation, large-scale processing, and archiving. This creates a **distributed computing continuum** spanning edge devices, observatories, institutional clusters, and global cloud platforms.\n",
    "\n",
    "By the early 2020s, seismology had fully entered the era of **continuous, automated, global-scale analysis**, in which data volume is no longer the primary limiter of scientific ambition. Instead, the dominant challenges are increasingly **computational cost, algorithmic transparency, software complexity, model bias, and long-term sustainability of cyberinfrastructure**. The field now operates as a truly **planetary-scale, real-time, computational Earth observatory**, built upon four decades of cumulative advances in computing hardware, networking, and digital data systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Instrumentation Sidebar: From Analog Seismographs to Nodal Arrays, DAS, and Citizen Seismology\n",
    "\n",
    "Modern digital seismology rests on more than a century of instrumentation development. The classical seismometer associated with **Charles Richter** in the 1930s–1940s was the **short-period electromagnetic seismometer**, designed to record local earthquakes with high sensitivity over a narrow frequency band (typically ~1 Hz). These instruments were coupled to **analog telemetry** and recorded on **mechanical or photographic drum helicorders**, producing continuous paper records that analysts visually inspected and measured by hand for phase arrival times and amplitudes. For several decades, this analog short-period instrumentation defined routine seismic monitoring worldwide.\n",
    "\n",
    "A major global transformation followed **World War II and the Manhattan Project**, as the ensuing **nuclear arms race** created an urgent geopolitical need to detect and locate underground nuclear tests. This directly led to the development of the **World-Wide Standardized Seismograph Network (WWSSN)** in the early **1960s**, a globally distributed array of **identically calibrated short-period and long-period analog seismographs** installed at hundreds of stations worldwide. For the first time, seismologists had access to **uniform, globally comparable seismic data**. The resulting global earthquake catalogs revealed the now-famous patterns of seismicity along ocean ridges, trenches, and transform faults, providing one of the decisive observational foundations of the **plate tectonics revolution of the late 1960s and early 1970s**.\n",
    "\n",
    "The next major instrumentation revolution came with the development of **portable broadband seismometers** in the **late 1980s**. These sensors used **capacitive feedback force-balance designs** rather than simple passive mass-spring systems, allowing a **much wider dynamic range and frequency bandwidth** (from long-period Earth tides to high-frequency local earthquakes) to be recorded **without mechanical saturation**. Early broadband instruments were first deployed in temporary research experiments, but by the **mid-1990s** they were increasingly adopted by permanent observatories. Their vastly increased dynamic range required a parallel shift from analog to **fully digital telemetry**, since analog links would clip the very signals the new sensors were designed to preserve.\n",
    "\n",
    "In the **2000s**, seismic digitizers themselves underwent a major transformation as they became **fully IP-addressable network devices**. Digitizers with native Ethernet, GPS timing, and embedded microprocessors could now be remotely configured, monitored, and integrated into standard Internet infrastructure. This enabled **true end-to-end digital seismic networks**, simplified field deployment, improved data reliability, and allowed real-time streaming directly into centralized archives and processing systems without custom telemetry hardware.\n",
    "\n",
    "The **2010s** saw another major shift with the rise of **large-N instrumentation**. **Nodal seismometers**—small, self-contained, battery-powered digital recorders—made it possible to deploy **hundreds to tens of thousands of sensors simultaneously**, transforming exploration and research seismology and enabling unprecedented spatial sampling of the seismic wavefield. At the same time, the emergence of **Distributed Acoustic Sensing (DAS)** repurposed fiber-optic cables as dense linear seismic arrays, allowing urban infrastructure, telecom cables, and subsea fibers to function as continent-scale seismic sensors with meter-scale spacing.\n",
    "\n",
    "Running in parallel with these professional networks, **citizen and educational seismology** expanded rapidly with the introduction of ultra-low-cost digital instruments. A key milestone was the launch of the **Raspberry Shake in 2016**, which combined a low-cost seismometer, digitizer, Linux computer, and Internet telemetry into a single compact unit. This enabled thousands of schools, hobbyists, and researchers worldwide to participate in real-time seismic monitoring and data sharing, dramatically expanding public engagement with seismology.\n",
    "\n",
    "Together, these successive instrumentation revolutions—from **analog short-period drums**, to **global standardized networks**, to **portable broadband sensors**, to **IP-based digitizers**, **nodal arrays**, **DAS**, and **citizen seismology**—have continually reshaped what is technically observable. Each step both responded to, and directly drove, the parallel revolutions in **telemetry, storage, computation, real-time processing, and large-scale data analysis** that define modern seismology.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 6. A Real Home Computer (1985)\n",
    "\n",
    "In **1985**, my dad bought a **BBC Micro Model B**. That was the computer I learned to program on as a kid. It had:\n",
    "\n",
    "- **32 kB of RAM**\n",
    "- **2 MHz CPU**\n",
    "- **No hard drive**\n",
    "- Programs loaded from a **cassette player**\n",
    "\n",
    "This consumer-grade computer was **~5–10 times faster than the Apollo 11 flight computer (1969)** in raw instruction speed — even though Apollo’s computer was far more reliable and mission-critical.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"bbcmodelb.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "And it ran amazing games like *Elite*:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"elite.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Scientific Workstations (Mid-1990s)\n",
    "\n",
    "In **1995**, when I was in grad school, I simulated synthetic seismograms on a **Sun SPARCstation 2**, which cost about **$15,000**.\n",
    "\n",
    "Compared to the BBC Micro, it was approximately:\n",
    "\n",
    "- **~1,500× faster in floating-point compute**\n",
    "- **~1,000× more RAM**\n",
    "- And it had a **real hard drive with fast disk I/O**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"sunsparc2.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 8. My Desktop Today\n",
    "\n",
    "Today, I use a **Mac mini M4**. Compared with the Sun SPARCstation I used at the start of my PhD ~30 years ago:\n",
    "\n",
    "- **Disk I/O:** ~2,000× faster  \n",
    "- **RAM:** ~1,000× larger  \n",
    "- **CPU:** ~30,000× faster  \n",
    "- **GPU:** ~7,000,000× faster  \n",
    "\n",
    "And this is still a *consumer-grade* machine — not HPC.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. The First Supercomputer (1976)\n",
    "\n",
    "In **1976**, when Steve McNutt was starting grad school, a huge leap forward was made: the world’s first true supercomputer — the **Cray-1** — was installed at **Los Alamos National Laboratory**.\n",
    "\n",
    "It originally cost **~$8–10 million in the 1970s** (roughly **$40–50 million today**).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"seymourcray1.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "Even so, the Cray-1 was only **~10× faster** than the Sun SPARCstation I would use about 20 years later.\n",
    "\n",
    "My **Mac mini M4 today is still**:\n",
    "\n",
    "- **~3,000× faster than a Cray-1 (CPU only)**\n",
    "- **Up to ~600,000× faster using the GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## 10. The World's Fastest Supercomputer Today\n",
    "\n",
    "Now fast-forward to today. The world’s fastest supercomputer is **El Capitan**, located at **Lawrence Livermore National Laboratory**.\n",
    "\n",
    "To match its performance, I would need **~28,000 Mac mini M4 GPUs running in parallel.**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"ElCapitan.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d1f8d",
   "metadata": {},
   "source": [
    "\n",
    "## Operating Systems & Programming Languages Sidebar (1970–2025): From Mainframes to Open-Source Scientific Computing\n",
    "\n",
    "### 1970s: Mainframes, Minicomputers, UNIX, and FORTRAN Dominance  \n",
    "In the **1970s**, scientific computing—including seismology—was dominated by **centralized mainframes and departmental minicomputers** running vendor-specific operating systems. The single most important development of the decade was the birth of **UNIX** at Bell Labs in **1969–1971**, alongside the creation of the **C programming language** to write it. UNIX introduced the ideas of a **portable operating system**, hierarchical file systems, pipes, and networked multi-user computing, all of which would later become foundational to scientific infrastructure.  \n",
    "\n",
    "For seismology, however, the dominant language was unmistakably **FORTRAN** (FORTRAN IV and later **FORTRAN 77**). Almost all earthquake location, magnitude, and travel-time codes—such as HYPO71 and related Geiger-method programs—were written in FORTRAN and ran in **batch mode** on IBM, CDC, and DEC systems. Programming was tightly coupled to numerical efficiency, since CPU time and memory were extremely limited.\n",
    "\n",
    "---\n",
    "\n",
    "### 1980s: MS-DOS, Early Windows, Mac OS, and the Rise of UNIX Workstations  \n",
    "The introduction of the **IBM PC (1981)** established **MS-DOS** as the dominant operating system for personal computing on **x86-compatible machines**. By the late 1980s, early versions of **Microsoft Windows** added a graphical layer on top of DOS, driving mass adoption in business, education, and administration.  \n",
    "\n",
    "In parallel, **classic Mac OS** (1984–2001) popularized the graphical user interface in universities for writing, graphics, and teaching, but Macs remained largely **disconnected from scientific UNIX computing** during this period.  \n",
    "\n",
    "For seismology and other physical sciences, the decisive platform of the 1980s was the **UNIX workstation** (Sun, SGI, DEC). These systems ran **native UNIX**, used **C and FORTRAN 77** for high-performance computing, and supported early digital telemetry and real-time seismic processing. The emergence of **automated STA/LTA detection, real-time acquisition, and waveform processing** was inseparable from the spread of UNIX-based systems.  \n",
    "\n",
    "**C** became the dominant systems programming language, while **FORTRAN 77** remained the primary language for numerical seismology.\n",
    "\n",
    "---\n",
    "\n",
    "### 1990s: UNIX Everywhere, Linux Emerges, and the First Open Scientific Ecosystem  \n",
    "The **1990s** marked the consolidation of **UNIX as the scientific operating system standard** and the emergence of **Linux (1991)** as a free, open-source UNIX-like OS. By the mid-to-late 1990s, Linux on commodity PCs began displacing proprietary UNIX workstations as the dominant scientific desktop and server platform.  \n",
    "\n",
    "At the same time, **Windows 95/98/NT** made Microsoft’s ecosystem dominant on undergraduate desktops and administrative systems, while **Mac OS** remained common for publishing and teaching. Crucially, however, Macs still lacked a true UNIX underpinnings until the next decade.  \n",
    "\n",
    "For programming languages, the 1990s saw:  \n",
    "- **C and FORTRAN 77/90** as the workhorses of real-time systems and numerical modeling  \n",
    "- The rise of **C++** for large software systems  \n",
    "- **Perl** becoming extremely important for **data parsing, automation, telemetry glue code, and early web services**  \n",
    "- The emergence of **MATLAB** as a powerful high-level numerical and visualization environment for research and teaching  \n",
    "\n",
    "Interpreted languages were widely used for scripting and data handling, but **performance-critical seismology remained dominated by compiled code** because CPU speed was still the principal bottleneck.\n",
    "\n",
    "---\n",
    "\n",
    "### 2000s: Linux Becomes the Scientific Default; Mac OS X Joins UNIX  \n",
    "The early **2000s** completed the transition to **Linux as the default operating system for scientific computing**, from desktops to clusters and national supercomputers. At the same time, a pivotal change occurred when Apple introduced **Mac OS X (2001)**—a fully **UNIX-based operating system with a native POSIX shell**. For the first time, Macs became *first-class scientific UNIX machines*, enabling seamless use of SSH, compilers, and open-source tools in seismological research.  \n",
    "\n",
    "Programming ecosystems in seismology diversified:  \n",
    "- **FORTRAN (77/90/95)** continued to dominate legacy modeling and wave-propagation codes  \n",
    "- **C and C++** remained central to real-time systems and large processing frameworks  \n",
    "- **MATLAB** became ubiquitous in teaching, signal processing, and visualization  \n",
    "- **Perl** remained critical for telemetry, web backends, and data formatting  \n",
    "\n",
    "By the end of the decade, the open-source UNIX–Linux–Mac ecosystem had largely unified scientific computing workflows across platforms.\n",
    "\n",
    "---\n",
    "\n",
    "### 2010s: Python, Open Science, and Platform Convergence  \n",
    "The **2010s** marked the decisive shift toward **high-level open scientific programming**, driven by **Python** and the maturation of open data infrastructures. With Linux, macOS, and cloud systems all presenting UNIX-like environments, platform differences became far less significant for researchers.  \n",
    "\n",
    "In seismology, **Python became transformative**, largely through the emergence of **ObsPy**, which standardized waveform I/O, metadata handling, catalog access, filtering, detection, and visualization in a single object-oriented framework. Python rapidly became the **dominant language for research workflows, pipeline development, and reproducible analysis**, while legacy FORTRAN and C/C++ codes increasingly served as back-end engines wrapped by Python interfaces.  \n",
    "\n",
    "Meanwhile:  \n",
    "- **Windows** remained dominant for general-purpose education and administration  \n",
    "- **Linux** dominated servers, clusters, and IRIS-style cyberinfrastructure  \n",
    "- **macOS** became a common UNIX research desktop  \n",
    "\n",
    "The field shifted culturally from locally maintained monolithic codes to **open-source, community-developed software ecosystems**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2020s: Cloud, ARM, AI/ML, and the Language Stack of Modern Seismology  \n",
    "By the **2020s**, scientific computing operated across a **heterogeneous but unified UNIX-like environment** spanning **Linux servers, macOS on ARM-based Apple Silicon, and cloud platforms**. Windows increasingly functioned primarily as a client interface rather than a core scientific backend.  \n",
    "\n",
    "The dominant language stack in modern seismology now consists of:  \n",
    "- **Python** for nearly all **research-level processing, machine learning, data discovery, and visualization**  \n",
    "- **C/C++ and FORTRAN** for **performance-critical kernels, real-time systems, and legacy modeling codes**  \n",
    "- **Java** for large enterprise-style processing systems and some real-time frameworks  \n",
    "- **MATLAB** continuing in education, signal-processing research, and legacy labs  \n",
    "\n",
    "In this period, **developer productivity, reproducibility, and scalability** have overtaken raw execution speed as the main constraints on progress. Whereas the 1980s and 1990s prioritized **machine efficiency**, the 2010s–2020s prioritize **human efficiency**, collaborative software engineering, and sustainable cyberinfrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "### Big-Picture Summary for Seismology\n",
    "\n",
    "- **1970s–1980s:** FORTRAN + vendor OS → batch processing → early real-time UNIX systems  \n",
    "- **1990s:** UNIX + Linux + C/FORTRAN dominate operations; Perl and MATLAB emerge  \n",
    "- **2000s:** Linux becomes standard; Mac OS X brings UNIX to Apple; mixed-language ecosystems stabilize  \n",
    "- **2010s–2020s:** Python + open-source frameworks (ObsPy) become the primary research interface; compiled languages move into the backend  \n",
    "\n",
    "Across five decades, seismology has evolved from **machine-centric computing constrained by hardware limits** to **human-centric computing constrained by data volume, algorithm design, and software sustainability**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc200f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Seismic Instrumentation Sidebar: From Analog Seismographs to Nodal Arrays, DAS, and Citizen Seismology\n",
    "\n",
    "Modern digital seismology rests on more than a century of instrumentation development. The classical seismometer associated with **Charles Richter** in the 1930s–1940s was the **short-period electromagnetic seismometer**, designed to record local earthquakes with high sensitivity over a narrow frequency band (typically ~1 Hz). These instruments were coupled to **analog telemetry** and recorded on **mechanical or photographic drum helicorders**, producing continuous paper records that analysts visually inspected and measured by hand for phase arrival times and amplitudes. For several decades, this analog short-period instrumentation defined routine seismic monitoring worldwide.\n",
    "\n",
    "A major global transformation followed **World War II and the Manhattan Project**, as the ensuing **nuclear arms race** created an urgent geopolitical need to detect and locate underground nuclear tests. This directly led to the development of the **World-Wide Standardized Seismograph Network (WWSSN)** in the early **1960s**, a globally distributed array of **identically calibrated short-period and long-period analog seismographs** installed at hundreds of stations worldwide. For the first time, seismologists had access to **uniform, globally comparable seismic data**. The resulting global earthquake catalogs revealed the now-famous patterns of seismicity along ocean ridges, trenches, and transform faults, providing one of the decisive observational foundations of the **plate tectonics revolution of the late 1960s and early 1970s**.\n",
    "\n",
    "The next major instrumentation revolution came with the development of **portable broadband seismometers** in the **late 1980s**. These sensors used **capacitive feedback force-balance designs** rather than simple passive mass-spring systems, allowing a **much wider dynamic range and frequency bandwidth** (from long-period Earth tides to high-frequency local earthquakes) to be recorded **without mechanical saturation**. Early broadband instruments were first deployed in temporary research experiments, but by the **mid-1990s** they were increasingly adopted by permanent observatories. Their vastly increased dynamic range required a parallel shift from analog to **fully digital telemetry**, since analog links would clip the very signals the new sensors were designed to preserve.\n",
    "\n",
    "In the **2000s**, seismic digitizers themselves underwent a major transformation as they became **fully IP-addressable network devices**. Digitizers with native Ethernet, GPS timing, and embedded microprocessors could now be remotely configured, monitored, and integrated into standard Internet infrastructure. This enabled **true end-to-end digital seismic networks**, simplified field deployment, improved data reliability, and allowed real-time streaming directly into centralized archives and processing systems without custom telemetry hardware.\n",
    "\n",
    "The **2010s** saw another major shift with the rise of **large-N instrumentation**. **Nodal seismometers**—small, self-contained, battery-powered digital recorders—made it possible to deploy **hundreds to tens of thousands of sensors simultaneously**, transforming exploration and research seismology and enabling unprecedented spatial sampling of the seismic wavefield. At the same time, the emergence of **Distributed Acoustic Sensing (DAS)** repurposed fiber-optic cables as dense linear seismic arrays, allowing urban infrastructure, telecom cables, and subsea fibers to function as continent-scale seismic sensors with meter-scale spacing.\n",
    "\n",
    "Running in parallel with these professional networks, **citizen and educational seismology** expanded rapidly with the introduction of ultra-low-cost digital instruments. A key milestone was the launch of the **Raspberry Shake in 2016**, which combined a low-cost seismometer, digitizer, Linux computer, and Internet telemetry into a single compact unit. This enabled thousands of schools, hobbyists, and researchers worldwide to participate in real-time seismic monitoring and data sharing, dramatically expanding public engagement with seismology.\n",
    "\n",
    "Together, these successive instrumentation revolutions—from **analog short-period drums**, to **global standardized networks**, to **portable broadband sensors**, to **IP-based digitizers**, **nodal arrays**, **DAS**, and **citizen seismology**—have continually reshaped what is technically observable. Each step both responded to, and directly drove, the parallel revolutions in **telemetry, storage, computation, real-time processing, and large-scale data analysis** that define modern seismology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106395d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flovopy_plus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
